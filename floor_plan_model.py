# -*- coding: utf-8 -*-
"""floor plan model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HF1g1LicJUWDdFJvrHV4MVnz3blqRZKX

# üèóÔ∏è Floorplan SDXL LoRA Training Notebook  
A clean, organized workflow for training, validating, saving, and exporting LoRA models.

# üì¶ Step 1: Install & Import Dependencies

In this step, we install all required libraries and import essential modules for
training, validation, and inference.  
We ensure GPU is enabled and prepare the environment for Stable Diffusion XL training.
"""

!pip uninstall -y diffusers accelerate transformers peft

!pip install diffusers==0.29.2
!pip install transformers==4.41.2
!pip install accelerate==0.27.2
!pip install peft==0.10.0
!pip install bitsandbytes safetensors einops

!wget https://raw.githubusercontent.com/huggingface/diffusers/v0.29.2/examples/text_to_image/train_text_to_image_lora_sdxl.py

!pip install diffusers==0.29.2 transformers accelerate peft==0.10.0 bitsandbytes safetensors datasets

import diffusers, accelerate, peft
print("DIFFUSERS:", diffusers.__version__)
print("ACCELERATE:", accelerate.__version__)
print("PEFT:", peft.__version__)

"""# üóÇÔ∏è Step 2: Load & Inspect Dataset

We load the dataset and verify that the CSV and image files are properly aligned.

"""

from google.colab import drive
drive.mount('/content/drive')

#/content/drive/MyDrive/final_dataset.zip
!unzip "/content/drive/MyDrive/final_dataset.zip" -d "/content/final_dataset"

import pandas as pd
import re

INPUT_CSV = "/content/final_dataset/resized_dataset.csv"
df = pd.read_csv(INPUT_CSV)
df.head()

"""# üßπ Step 3 ‚Äî Text Preprocessing

Before training the model, we clean and standardize all text captions.  
This makes the dataset more consistent and helps the model learn more efficiently.

## üîß What happens in this step:

- Remove extra spaces  
- Convert all text to lowercase  
- Fix common formatting issues  
- Ensure each caption matches the correct image  


After preprocessing, the captions become cleaner, more uniform, and better suited for training.

"""

import pandas as pd
import re

INPUT_CSV = "/content/final_dataset/resized_dataset.csv"
OUTPUT_CSV = "/content/final_clean_dataset.csv"

df = pd.read_csv(INPUT_CSV)

def clean_text_keep_everything(raw):
    if not isinstance(raw, str):
        return ""

    text = raw

    # remove weird unicode characters
    text = text.encode('ascii', 'ignore').decode()

    # normalize underscores
    text = text.replace("_", " ")

    # add spaces around separators
    text = re.sub(r"\|", " | ", text)

    # normalize parentheses spacing
    text = re.sub(r"\(", " (", text)
    text = re.sub(r"\)", ") ", text)

    # collapse multiple spaces
    text = re.sub(r"\s+", " ", text)

    text = text.replace("|", ",")

    # strip leading and trailing spaces
    text = text.strip()

    # ensure ending period
    if not text.endswith("."):
        text += "."

    return text


# Create final caption for each row
df["text"] = df["Text"].apply(clean_text_keep_everything)

# Keep only required columns
df = df[["image", "text"]]

df.to_csv(OUTPUT_CSV, index=False)
print("‚úî Final cleaned CSV saved to:", OUTPUT_CSV)

df.head()

DATA_DIR = "/content/final_dataset/resized_images"
CAPTION_CSV = "/content/final_clean_dataset.csv"
OUTPUT_DIR = "/content/drive/MyDrive/floorplan-lora"

import pandas as pd
import re

df = pd.read_csv("/content/final_clean_dataset.csv")

df["image"] = df["image"].astype(str)

df["text"] = df["text"].astype(str)

df.head()
df.dtypes

import pandas as pd
import os

CSV_PATH = "/content/final_clean_dataset.csv"
IMAGE_DIR = "/content/final_dataset/resized_images"

df = pd.read_csv(CSV_PATH)

for _, row in df.iterrows():
    image_path = row["image"].replace("\\", "/")
    caption = row["text"]

    name = os.path.basename(image_path)
    base = os.path.splitext(name)[0]

    with open(os.path.join(IMAGE_DIR, base + ".txt"), "w") as f:
        f.write(caption)

print("‚úîÔ∏è Captions created!")

from datasets import Dataset
train_data = Dataset.from_pandas(df)
train_data

import pandas as pd, shutil, os

df = pd.read_csv("/content/final_clean_dataset.csv")

# 1. Fix Windows backslashes
df["image"] = df["image"].str.replace("\\", "/", regex=False)

# 2. Add the correct folder prefix where your images really are
df["image"] = "/content/final_dataset/" + df["image"]

os.makedirs("train_data", exist_ok=True)

missing = 0

for i, row in df.iterrows():
    src = row["image"]
    new_name = f"{i}.png"
    dst = f"train_data/{new_name}"

    if os.path.exists(src):
        shutil.copy(src, dst)
        df.loc[i, "image"] = new_name
    else:
        print("Missing:", src)
        missing += 1

df[["image","text"]].to_csv("train_data/metadata.csv", index=False)

print("DONE! Missing files:", missing)

import pandas as pd
import shutil, os

df = pd.read_csv("/content/final_clean_dataset.csv")

# Fix slashes AND point to the real folder
df["image"] = "/content/final_dataset/" + df["image"].str.replace("\\", "/", regex=False)

os.makedirs("images", exist_ok=True)

missing = 0
for path in df["image"]:
    if os.path.exists(path):
        shutil.copy(path, "images/")
    else:
        print("Missing:", path)
        missing += 1

print("Total missing:", missing)

df.to_csv("metadata.csv", index=False)

import pandas as pd, shutil, os

df = pd.read_csv("/content/final_clean_dataset.csv")

os.makedirs("final_train", exist_ok=True)

# List all real images in train_data
real_images = sorted([f for f in os.listdir("train_data") if f.lower().endswith((".png",".jpg",".jpeg"))])

if len(real_images) != len(df):
    print("WARNING: Image count and CSV rows do NOT match!")
    print("CSV rows:", len(df))
    print("Images found:", len(real_images))

# Rename all images 0.png, 1.png, 2.png...
for i, real_image in enumerate(real_images):
    new_name = f"{i}.png"
    shutil.copy(f"train_data/{real_image}", f"final_train/{new_name}")
    df.loc[i, "image"] = new_name

# Save metadata
df[["image","text"]].to_csv("final_train/metadata.csv", index=False)

print("DONE! Images renamed + metadata created.")

import pandas as pd
pd.read_csv("train_data/metadata.csv").head()

import pandas as pd

df = pd.read_csv("train_data/metadata.csv")
df.rename(columns={"image": "file_name"}, inplace=True)
df.to_csv("train_data/metadata.csv", index=False)

print(df.head())

from datasets import Dataset
train_data = Dataset.from_pandas(df)
train_data

"""# üöÄ Step 4: Start Training LoRA

This cell launches the actual LoRA training using Accelerate.
During training:
- Checkpoints are saved every 500 steps  
- Step loss is printed  
- GPU utilization is monitored  

If the notebook disconnects, you can resume training from the latest checkpoint.

"""

!accelerate launch train_text_to_image_lora_sdxl.py \
  --pretrained_model_name_or_path="stabilityai/stable-diffusion-xl-base-1.0" \
  --train_data_dir="train_data" \
  --caption_column="text" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --learning_rate=1e-4 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_train_epochs=6  \
  --output_dir="floorplan_lora" \
  --checkpointing_steps=500 \
  --output_dir="/content/drive/MyDrive/floorplan_lora" \
  --mixed_precision="fp16" \
  --use_8bit_adam

"""# üîç Step 5: Validate LoRA Checkpoints

Here we load each checkpoint (500, 1000, 1500, 2000)  
and generate one test image.  
This helps determine which checkpoint gives the best quality output.

"""

from diffusers import StableDiffusionXLPipeline
import torch

pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16
).to("cuda")

pipe.load_lora_weights("/content/floorplan_lora/pytorch_lora_weights.safetensors")

prompt = "bedrooms : 5 , toilet : 3 , attached toilet : 3 , dress room : 0 , features :living room, dining, kitchen"

image = pipe(
    prompt,
    num_inference_steps=60,   # Higher = better quality
    guidance_scale=7.5          # Controls prompt strength
).images[0]

image.save("test.png")

"""# üèÖ Step 6: Select the Best Checkpoint

After testing all checkpoints, choose the best performing one based on:
- Image clarity  
- Room count accuracy  
- Door/window accuracy  
- Line consistency  

"""

from diffusers import StableDiffusionXLPipeline
import torch
import os

# ----------------------
# PATHS
# ----------------------
BASE_MODEL = "stabilityai/stable-diffusion-xl-base-1.0"
LORA_DIR = "/content/floorplan_lora"

CHECKPOINTS = {
    "final": f"{LORA_DIR}/pytorch_lora_weights.safetensors",
    "500": f"{LORA_DIR}/checkpoint-500/pytorch_lora_weights.safetensors",
    "1000": f"{LORA_DIR}/checkpoint-1000/pytorch_lora_weights.safetensors",
    "1500": f"{LORA_DIR}/checkpoint-1500/pytorch_lora_weights.safetensors",
    "2000": f"{LORA_DIR}/checkpoint-2000/pytorch_lora_weights.safetensors",
}

SAVE_DIR = "/content/lora_comparison"
os.makedirs(SAVE_DIR, exist_ok=True)

# ----------------------
# Load Base Pipeline
# ----------------------
pipe = StableDiffusionXLPipeline.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
).to("cuda")

prompt = "bedrooms : 3 , toilet : 2 , attached toilet : 1 , dress room : 0 , features :drawing room, dining, kitchen, rear lawn."

# ----------------------
# Generate images for each checkpoint
# ----------------------
for name, ckpt_path in CHECKPOINTS.items():
    print(f"\nüî• Generating for checkpoint: {name}")

    # Load LoRA
    pipe.unload_lora_weights()  # Clear previous LoRA
    pipe.load_lora_weights(ckpt_path)

    # Generate image
    image = pipe(
        prompt,
        num_inference_steps=50,
        guidance_scale=7.5
    ).images[0]

    save_path = f"{SAVE_DIR}/{name}.png"
    image.save(save_path)
    print(f"‚úîÔ∏è Saved: {save_path}")

print("\nüéâ DONE! All comparison images saved in /content/lora_comparison")

"""# üåê Step 7: Build a Flask API

we deploy a Flask API that receives JSON requests
and returns generated floorplan images.  
Flutter can call the API instead of running SDXL locally.

"""

!pip install diffusers transformers accelerate safetensors --upgrade

import os

path = "/content/drive/MyDrive/floorplan_lora"
print(os.listdir(path))
# before resume

!pip uninstall -y diffusers accelerate transformers peft

!pip install diffusers==0.29.2
!pip install transformers==4.41.2
!pip install accelerate==0.27.2
!pip install peft==0.10.0
!pip install bitsandbytes safetensors einops

!wget https://raw.githubusercontent.com/huggingface/diffusers/v0.29.2/examples/text_to_image/train_text_to_image_lora_sdxl.py

!pip install diffusers==0.29.2 transformers accelerate peft==0.10.0 bitsandbytes safetensors datasets

import torch
print(torch.cuda.is_available())

import torch, gc
from diffusers import DiffusionPipeline

from diffusers import StableDiffusionXLPipeline
import torch
from PIL import Image
import os, glob
from pathlib import Path

!pip install flask pyngrok diffusers transformers accelerate safetensors torch torchvision

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, request, jsonify, send_from_directory
# import torch
# from diffusers import DiffusionPipeline
# 
# app = Flask(__name__)
# 
# print("Loading SDXL pipeline...")
# 
# base_model = "stabilityai/stable-diffusion-xl-base-1.0"
# pipe = DiffusionPipeline.from_pretrained(
#     base_model,
#     torch_dtype=torch.float16,
#     variant="fp16"
# ).to("cuda")
# 
# lora_path = "/content/drive/MyDrive/floorplan_lora/pytorch_lora_weights.safetensors"
# pipe.load_lora_weights(lora_path)
# 
# print("Pipeline loaded with LoRA.")
# 
# @app.route("/", methods=["GET"])
# def home():
#     return "SDXL LoRA API is running!"
# 
# @app.route("/generate", methods=["POST"])
# def generate():
#     data = request.get_json()
# 
#     prompt = data.get("prompt")
#     width = int(data.get("width", 1024))
#     height = int(data.get("height", 1024))
# 
#     if not prompt:
#         return jsonify({"error": "Prompt is required"}), 400
# 
#     # Run SDXL + LoRA
#     image = pipe(
#         prompt=prompt,
#         width=width,
#         height=height,
#         num_inference_steps=30,
#     ).images[0]
# 
#     filename = "generated.png"
#     image.save(filename)
# 
#     # Make the file accessible via ngrok
#     public_url = request.host_url + filename
# 
#     return jsonify({
#         "message": "Image generated successfully",
#         "url": public_url
#     })
# 
# @app.route('/<path:filename>', methods=['GET'])
# def serve_file(filename):
#     return send_from_directory(".", filename)
# 
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=5000)
#

!ngrok config add-authtoken 

from pyngrok import ngrok
public_url = ngrok.connect(5000)
print(public_url)
!python app.py;

"""üìå Step 8 ‚Äî Resume Training & Improve Model Quality

In this step, we continue training the SDXL LoRA model from an existing checkpoint (2000 steps).
This allows the model to keep learning while preserving what it already understood before.

We load the dataset again, unpack the saved project folder, and resume training with optimized settings such as:

Gradient accumulation ‚Üí stabilizes training on small batch size

FP16 mixed precision ‚Üí faster + less GPU memory

8-bit Adam optimizer ‚Üí reduced memory usage

Regular checkpoint saving ‚Üí ensures we can always resume training safely

This process helps refine the model and improve the quality of generated floorplans.
"""

!unzip /content/drive/MyDrive/floorplan_lora-20251126T133920Z-1-001.zip

!accelerate launch train_text_to_image_lora_sdxl.py \
  --pretrained_model_name_or_path="stabilityai/stable-diffusion-xl-base-1.0" \
  --train_data_dir="train_data" \
  --caption_column="text" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=8 \
  --learning_rate=5e-5 \
  --max_train_steps=6000 \
  --resume_from_checkpoint="checkpoint-2000" \
  --checkpointing_steps=500 \
  --output_dir="/content/drive/MyDrive/floorplan_lora" \
  --mixed_precision="fp16" \
  --use_8bit_adam